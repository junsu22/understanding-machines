  
### LDA · 로지스틱 회귀 · 신경망 정리


---

## Fisher의 선형 판별분석 (LDA)

**Fisher의 선형 판별분석(LDA)**은  
데이터를 잘 구분할 수 있는 방향을 찾으면서  
동시에 차원을 축소하는 방법이다.

LDA의 핵심 아이디어는 다음과 같다.

- 같은 클래스에 속한 데이터들은 최대한 **가깝게 모이도록**
- 서로 다른 클래스에 속한 데이터들은 최대한 **멀어지도록**

이 두 조건을 가장 잘 만족시키는 **판별 방향**을 찾는 것이 목적이다.

하지만 판별에 유리한 방향을 찾았다고 해서  
데이터가 자동으로 비교 가능한 형태가 되는 것은 아니다.  
데이터는 여전히 다차원 공간에 흩어져 있기 때문이다.

그래서 LDA에서는  
데이터를 해당 판별 방향으로 **정사영(projection)** 한다.  
이 과정에서 판별 방향에 직교한 정보는 제거되고,  
클래스 구분에 중요한 성분만 남게 된다.  
그 결과, 각 데이터는 하나의 **판별값**으로 표현될 수 있다.

---

## 라그랑주 승수법과 LDA

LDA를 수식으로 풀어가다 보면  
**라그랑주 승수법**이 등장한다.

라그랑주 승수법은  
제약 조건이 있는 상태에서 최대값이나 최소값을 찾기 위한 방법이다.

LDA에서 판별 방향 벡터는  
그 크기를 키우기만 해도 분산 값이 함께 커진다.  
아무 제약 없이 최대화를 하면  
의미 있는 ‘방향’이 아니라  
단순히 벡터의 크기만 커지는 해가 나올 수 있다.

이를 막기 위해  
벡터의 크기를 고정하는 제약 조건을 두고,  
그 조건 안에서 방향만 비교하도록 문제를 재정의한다.  

라그랑주 승수법은  
**문제의 의미는 유지한 채, 해가 잘 정의되도록 문제를 바꿔주는 역할**을 한다.

---

## 로지스틱 회귀

**로지스틱 회귀**는  
분류 문제를 **확률 문제로 변환**해서 푸는 방법이다.

입력값에 대해  
시그모이드 함수를 적용하여  
0과 1 사이의 확률값을 계산하고,  
보통 0.5를 기준으로  
최종적으로 0 또는 1로 분류한다.

아래 그래프는 Plotly를 이용해  
선형 회귀와 로지스틱 회귀를 같은 데이터에 적용해 비교한 결과이다.
![](https://velog.velcdn.com/images/junsu22/post/2499e63f-1678-44ea-a7b1-efebc62c64d5/image.png)

- 선형 회귀는 직선 형태이기 때문에  
  예측값이 0보다 작아지거나 1보다 커질 수 있다.
- 반면 로지스틱 회귀는  
  출력값을 항상 0과 1 사이의 확률로 제한한다.

이 시각화를 통해  
**선형 회귀는 연속값 예측에는 적합하지만,  
이진 분류 문제에는 로지스틱 회귀가 더 자연스럽다**는 점을  
직관적으로 이해할 수 있었다.

---

## 신경망과 기계학습

**기계학습(Machine Learning)**은  
데이터로부터 패턴을 학습하여  
예측이나 분류를 수행하는 방법이다.

**신경망(Neural Network)**은  
이러한 기계학습의 한 방법으로,  
생물학적 뉴런을 모방한 인공 뉴런들이  
층(layer) 구조로 연결된 모델이다.

신경망은  
입력층, 은닉층, 출력층으로 구성되며,  
각 뉴런은 가중치를 가진 연결을 통해 정보를 전달한다.  
여러 층을 깊게 쌓은 신경망을  
**딥러닝(Deep Learning)**이라고 한다.

---

## 인공신경망의 원리

생물학적 뉴런이  
임계값 이상의 자극에만 반응하듯,  
인공 뉴런도 비슷한 방식으로 작동한다.

1. 다수의 입력 신호를 받는다.
2. 각 입력에 가중치를 곱해 가중합을 계산한다.
3. 가중합을 활성화 함수에 통과시켜 출력값을 결정한다.

이 과정을 통해  
입력 데이터는 다음 층으로 전달되며,  
점차 의미 있는 표현으로 변환된다.

---

## 오차역전파법 (Backpropagation)

신경망 학습의 핵심은 **오차역전파법**이다.

신경망은  
예측값과 실제 정답 사이의 오차를 계산한 뒤,  
그 오차를 출력층에서 입력층 방향으로 거꾸로 전파한다.  

이 과정에서 각 가중치가  
오차에 얼마나 영향을 미쳤는지를 계산하고,  
경사하강법을 이용해 가중치를 조금씩 수정한다.

이 과정을 반복하면서  
신경망은 점점 오차를 줄이는 방향으로 학습하게 된다.

---

## 마무리하며


- LDA는 **방향을 찾는 문제**로,
- 로지스틱 회귀는 **확률 문제**로,
- 신경망은 **표현을 학습하는 문제**로 바라본다는 점이 흥미로웠다.

